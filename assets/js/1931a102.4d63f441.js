"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[5785],{2180:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/capstone-project","title":"Capstone Project: Autonomous Humanoid","description":"Build a complete autonomous humanoid robot system integrating all course concepts.","source":"@site/docs-software/module-4/capstone-project.md","sourceDirName":"module-4","slug":"/module-4/capstone-project","permalink":"/physical-ai-book/docs-software/module-4/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/sufyanarain/physical-ai-book/tree/main/website/docs-software/module-4/capstone-project.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action (VLA)","permalink":"/physical-ai-book/docs-software/module-4/vla-intro"}}');var i=t(4848),a=t(8453);const s={sidebar_position:2},r="Capstone Project: Autonomous Humanoid",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Environment Setup",id:"step-1-environment-setup",level:3},{value:"Step 2: Voice Command Handler",id:"step-2-voice-command-handler",level:3},{value:"Step 3: Task Planner",id:"step-3-task-planner",level:3},{value:"Step 4: Navigation Module",id:"step-4-navigation-module",level:3},{value:"Step 5: Object Detection",id:"step-5-object-detection",level:3},{value:"Step 6: Manipulation Controller",id:"step-6-manipulation-controller",level:3},{value:"Step 7: Main Controller",id:"step-7-main-controller",level:3},{value:"Launch File",id:"launch-file",level:2},{value:"Demo Scenarios",id:"demo-scenarios",level:2},{value:"Scenario 1: &quot;Clean the room&quot;",id:"scenario-1-clean-the-room",level:3},{value:"Scenario 2: &quot;Bring me water&quot;",id:"scenario-2-bring-me-water",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Congratulations! \ud83c\udf89",id:"congratulations-",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-autonomous-humanoid",children:"Capstone Project: Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.p,{children:"Build a complete autonomous humanoid robot system integrating all course concepts."}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"Create a simulated humanoid robot that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Receives voice commands"}),' (e.g., "Clean the room") using a voice recognition system, such as OpenAI Whisper API, which converts spoken language into text.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plans a sequence of actions"})," using a Large Language Model (LLM) like GPT-4, which generates a plan based on the voice command."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigates obstacles"})," using Visual Simultaneous Localization and Mapping (VSLAM) and Navigation2 (Nav2), which enable the robot to move around and avoid collisions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Identifies objects"})," with computer vision using YOLO (You Only Look Once) or other object detection algorithms, which allow the robot to recognize and classify objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulates objects"})," with arm control using MoveIt or other motion planning libraries, which enable the robot to pick up and move objects."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Voice Command Input                \u2502\n\u2502         (OpenAI Whisper API)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Cognitive Planning Layer                \u2502\n\u2502         (GPT-4 Task Decomposition)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Perception    \u2502  \u2502   Navigation    \u2502\n\u2502  (YOLO/Isaac)   \u2502  \u2502   (Nav2/VSLAM)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Action Execution Layer                \u2502\n\u2502     (ROS 2 Controllers + Gazebo/Isaac)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.p,{children:"Think of this architecture like a software system, where each layer is a separate module that communicates with the others. The voice command input is like a user interface, the cognitive planning layer is like a business logic layer, and the action execution layer is like a database or storage layer."}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-environment-setup",children:"Step 1: Environment Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create workspace\nmkdir -p ~/humanoid_project/src\ncd ~/humanoid_project\n\n# Clone humanoid model (e.g., Unitree G1)\ngit clone https://github.com/unitreerobotics/unitree_ros2.git src/\n\n# Build\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(n.p,{children:"This step is like setting up a development environment for a software project. You create a workspace, clone a repository, and build the project."}),"\n",(0,i.jsx)(n.h3,{id:"step-2-voice-command-handler",children:"Step 2: Voice Command Handler"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# voice_handler.py\nimport whisper\nfrom openai import OpenAI\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass VoiceHandler(Node):\n    def __init__(self):\n        super().__init__(\'voice_handler\')\n        self.whisper = whisper.load_model("base")\n        self.openai = OpenAI()\n        \n        # Publisher for commands\n        self.cmd_pub = self.create_publisher(String, \'voice_commands\', 10)\n        \n        # Timer for mic input\n        self.create_timer(5.0, self.listen)\n    \n    def listen(self):\n        # Capture audio (implement with pyaudio/sounddevice)\n        audio_file = self.record_audio(duration=3)\n        \n        # Transcribe\n        result = self.whisper.transcribe(audio_file)\n        command = result["text"]\n        \n        self.get_logger().info(f"Command: {command}")\n        \n        # Publish\n        self.cmd_pub.publish(String(data=command))\n\ndef main():\n    rclpy.init()\n    node = VoiceHandler()\n    rclpy.spin(node)\n'})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a voice-controlled interface for a software application. You use a library like Whisper to transcribe spoken language into text and then publish the command to a topic."}),"\n",(0,i.jsx)(n.h3,{id:"step-3-task-planner",children:"Step 3: Task Planner"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# task_planner.py\nfrom openai import OpenAI\nimport json\n\nclass TaskPlanner(Node):\n    def __init__(self):\n        super().__init__(\'task_planner\')\n        self.openai = OpenAI()\n        \n        # Subscribe to voice commands\n        self.cmd_sub = self.create_subscription(\n            String, \'voice_commands\', self.plan_callback, 10\n        )\n        \n        # Publisher for action sequence\n        self.action_pub = self.create_publisher(String, \'action_sequence\', 10)\n    \n    def plan_callback(self, msg):\n        command = msg.data\n        \n        # Generate action plan\n        plan = self.generate_plan(command)\n        \n        # Publish as JSON\n        self.action_pub.publish(String(data=json.dumps(plan)))\n    \n    def generate_plan(self, command):\n        prompt = f"""\n        Convert this command to a sequence of robot actions:\n        Command: "{command}"\n        \n        Available actions:\n        - navigate_to(x, y)\n        - detect_objects(category)\n        - pick_object(object_id)\n        - place_object(x, y, z)\n        - open_gripper()\n        - close_gripper()\n        \n        Return JSON array of actions with parameters.\n        """\n        \n        response = self.openai.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": "Robot task planner"},\n                {"role": "user", "content": prompt}\n            ]\n        )\n        \n        plan_text = response.choices[0].message.content\n        # Extract JSON from response\n        plan = json.loads(plan_text)\n        \n        return plan\n'})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a business logic layer for a software application. You use a library like OpenAI to generate a plan based on the voice command and then publish the plan to a topic."}),"\n",(0,i.jsx)(n.h3,{id:"step-4-navigation-module",children:"Step 4: Navigation Module"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# navigator.py\nfrom nav2_simple_commander.robot_navigator import BasicNavigator\nfrom geometry_msgs.msg import PoseStamped\n\nclass Navigator(Node):\n    def __init__(self):\n        super().__init__('navigator')\n        self.navigator = BasicNavigator()\n        \n        # Subscribe to navigation commands\n        self.nav_sub = self.create_subscription(\n            PoseStamped, 'navigate_to', self.navigate_callback, 10\n        )\n    \n    def navigate_callback(self, goal_pose):\n        self.get_logger().info(f'Navigating to: {goal_pose.pose.position}')\n        \n        # Set initial pose\n        initial_pose = PoseStamped()\n        initial_pose.header.frame_id = 'map'\n        initial_pose.pose.position.x = 0.0\n        initial_pose.pose.position.y = 0.0\n        self.navigator.setInitialPose(initial_pose)\n        \n        # Wait for Nav2 to activate\n        self.navigator.waitUntilNav2Active()\n        \n        # Send goal\n        self.navigator.goToPose(goal_pose)\n        \n        # Monitor progress\n        while not self.navigator.isTaskComplete():\n            feedback = self.navigator.getFeedback()\n            self.get_logger().info(f'Distance remaining: {feedback.distance_remaining}')\n        \n        result = self.navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            self.get_logger().info('Navigation succeeded!')\n        else:\n            self.get_logger().error('Navigation failed!')\n"})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a database or storage layer for a software application. You use a library like Nav2 to navigate to a goal pose and then monitor the progress."}),"\n",(0,i.jsx)(n.h3,{id:"step-5-object-detection",children:"Step 5: Object Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# object_detector.py\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nfrom ultralytics import YOLO\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n        self.bridge = CvBridge()\n        self.model = YOLO('yolov8n.pt')\n        \n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.detect_callback, 10\n        )\n        \n        # Publisher for detected objects\n        self.obj_pub = self.create_publisher(String, 'detected_objects', 10)\n    \n    def detect_callback(self, msg):\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        \n        # Detect objects\n        results = self.model(cv_image)\n        \n        # Extract detections\n        detections = []\n        for result in results:\n            for box in result.boxes:\n                detection = {\n                    'class': result.names[int(box.cls)],\n                    'confidence': float(box.conf),\n                    'bbox': box.xyxy[0].tolist()\n                }\n                detections.append(detection)\n        \n        # Publish\n        self.obj_pub.publish(String(data=json.dumps(detections)))\n"})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a data processing layer for a software application. You use a library like YOLO to detect objects in an image and then publish the detections to a topic."}),"\n",(0,i.jsx)(n.h3,{id:"step-6-manipulation-controller",children:"Step 6: Manipulation Controller"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# manipulator.py\nfrom moveit_msgs.action import MoveGroup\nfrom rclpy.action import ActionClient\n\nclass Manipulator(Node):\n    def __init__(self):\n        super().__init__('manipulator')\n        \n        # MoveIt action client\n        self.move_client = ActionClient(self, MoveGroup, 'move_action')\n        \n        # Subscribe to manipulation commands\n        self.manip_sub = self.create_subscription(\n            String, 'manipulation_command', self.manip_callback, 10\n        )\n    \n    def manip_callback(self, msg):\n        command = json.loads(msg.data)\n        \n        if command['action'] == 'pick_object':\n            self.pick(command['object_id'])\n        elif command['action'] == 'place_object':\n            self.place(command['position'])\n    \n    def pick(self, object_id):\n        # Get object position\n        position = self.get_object_position(object_id)\n        \n        # Plan grasp\n        goal = MoveGroup.Goal()\n        goal.request.goal_constraints = self.create_grasp_constraints(position)\n        \n        # Execute\n        self.move_client.send_goal_async(goal)\n"})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a control layer for a software application. You use a library like MoveIt to plan and execute a grasp action."}),"\n",(0,i.jsx)(n.h3,{id:"step-7-main-controller",children:"Step 7: Main Controller"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# main_controller.py\nclass HumanoidController(Node):\n    def __init__(self):\n        super().__init__('humanoid_controller')\n        \n        # Subscribe to action sequence\n        self.action_sub = self.create_subscription(\n            String, 'action_sequence', self.execute_sequence, 10\n        )\n        \n        # Action publishers\n        self.nav_pub = self.create_publisher(PoseStamped, 'navigate_to', 10)\n        self.manip_pub = self.create_publisher(String, 'manipulation_command', 10)\n    \n    def execute_sequence(self, msg):\n        actions = json.loads(msg.data)\n        \n        for action in actions:\n            self.execute_action(action)\n            \n            # Wait for completion\n            time.sleep(1)\n    \n    def execute_action(self, action):\n        if action['type'] == 'navigate_to':\n            pose = self.create_pose(action['params'])\n            self.nav_pub.publish(pose)\n        \n        elif action['type'] == 'pick_object':\n            cmd = json.dumps(action)\n            self.manip_pub.publish(String(data=cmd))\n"})}),"\n",(0,i.jsx)(n.p,{children:"This step is like creating a main loop for a software application. You subscribe to an action sequence topic and then execute each action in the sequence."}),"\n",(0,i.jsx)(n.h2,{id:"launch-file",children:"Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# humanoid_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Gazebo/Isaac Sim\n        Node(\n            package='gazebo_ros',\n            executable='gazebo',\n            arguments=['humanoid_world.world']\n        ),\n        \n        # Voice handler\n        Node(package='humanoid_project', executable='voice_handler'),\n        \n        # Task planner\n        Node(package='humanoid_project', executable='task_planner'),\n        \n        # Navigator\n        Node(package='humanoid_project', executable='navigator'),\n        \n        # Object detector\n        Node(package='humanoid_project', executable='object_detector'),\n        \n        # Manipulator\n        Node(package='humanoid_project', executable='manipulator'),\n        \n        # Main controller\n        Node(package='humanoid_project', executable='main_controller'),\n    ])\n"})}),"\n",(0,i.jsx)(n.p,{children:"This launch file is like a configuration file for a software application. You define the nodes that should be launched and their parameters."}),"\n",(0,i.jsx)(n.h2,{id:"demo-scenarios",children:"Demo Scenarios"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-1-clean-the-room",children:'Scenario 1: "Clean the room"'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice"}),': "Clean the room"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": [navigate_to(room), detect_objects(trash), pick_object(trash_1), navigate_to(bin), place_object(bin)]"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execute"}),": Robot navigates, detects trash, picks it up, moves to bin, drops it"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"scenario-2-bring-me-water",children:'Scenario 2: "Bring me water"'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice"}),': "Bring me water"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": [navigate_to(kitchen), detect_objects(bottle), pick_object(bottle_1), navigate_to(user), place_object(table)]"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execute"}),": Fetches and delivers water"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice recognition accuracy"})," (greater than 90%)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task completion rate"})," (greater than 80%)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation safety"})," (0 collisions)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object detection precision"})," (greater than 85%)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution time"})," (less than 5 minutes per task)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 Integrated all 4 modules into one system",(0,i.jsx)(n.br,{}),"\n","\u2705 Voice-to-action complete pipeline",(0,i.jsx)(n.br,{}),"\n","\u2705 Real-world task execution",(0,i.jsx)(n.br,{}),"\n","\u2705 Modular, scalable architecture"]}),"\n",(0,i.jsx)(n.h2,{id:"congratulations-",children:"Congratulations! \ud83c\udf89"}),"\n",(0,i.jsx)(n.p,{children:"You've completed the Physical AI & Humanoid Robotics textbook. You now have the skills to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Build robots with ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Simulate with Gazebo and Isaac"}),"\n",(0,i.jsx)(n.li,{children:"Integrate AI for perception and planning"}),"\n",(0,i.jsx)(n.li,{children:"Create voice-controlled autonomous systems"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Continue learning"})," and building the future of robotics!"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Join ",(0,i.jsx)(n.a,{href:"https://discourse.ros.org/",children:"ROS Discourse"})]}),"\n",(0,i.jsxs)(n.li,{children:["Contribute to ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"Isaac ROS"})]}),"\n",(0,i.jsx)(n.li,{children:"Build your own projects"}),"\n",(0,i.jsx)(n.li,{children:"Share with the community"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Questions?"})," Ask the chatbot! \u2192"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);